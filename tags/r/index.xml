<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Abhijit Dasgupta</title>
    <link>/tags/r/</link>
    <description>Recent content in R on Abhijit Dasgupta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Abhijit Dasgupta</copyright>
    <lastBuildDate>Thu, 20 Jul 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Quirks about running Rcpp on Windows through RStudio</title>
      <link>/post/quirks-about-running-rcpp-on-windows-through-rstudio/</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/quirks-about-running-rcpp-on-windows-through-rstudio/</guid>
      <description>Quirks about running Rcpp on Windows through RStudio This is a quick note about some tribulations I had running Rcpp (v. 0.12.12) code through RStudio (v. 1.0.143) on a Windows 7 box running R (v. 3.3.2). I also have RTools v. 3.4 installed. I fully admit that this may very well be specific to my box, but I suspect not.
I kept running into problems with Rcpp complaining that (a) RTools wasn&amp;rsquo;t installed, and (b) the C++ compiler couldn&amp;rsquo;t find Rcpp.</description>
    </item>
    
    <item>
      <title>pandas &#34;transform&#34; using the tidyverse</title>
      <link>/post/pandas-transform-using-the-tidyverse/</link>
      <pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/pandas-transform-using-the-tidyverse/</guid>
      <description>Chris Moffit has a nice blog on how to use the transform function in pandas. He provides some (fake) data on sales and asks the question of what fraction of each order is from each SKU.
Being a R nut and a tidyverse fan, I thought to compare and contrast the code for the pandas version with an implementation using the tidyverse.
First the pandas code:
import pandas as pd dat = pd.</description>
    </item>
    
    <item>
      <title>A quick exploration of the ReporteRs package</title>
      <link>/post/a-quick-exploration-of-reporters/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/a-quick-exploration-of-reporters/</guid>
      <description>The package ReporteRs has been getting some play on the interwebs this week, though it&amp;rsquo;s actually been around for a while. The nice thing about this package is that it allows writing Word and PowerPoint documents in an OS-independent fashion unlike some earlier packages. It also allows the editing of documents by using bookmarks within the documents.
This quick note is just to remind me that the structure of ReporteRs works beautifully with the piping conventions of magrittr.</description>
    </item>
    
    <item>
      <title>Creating new data with max values for each subject</title>
      <link>/post/creating-new-data-with-max-values-for-each-subject-2/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/creating-new-data-with-max-values-for-each-subject-2/</guid>
      <description>We have a data set dat with multiple observations per subject. We want to create a subset of this data such that each subject (with ID giving the unique identifier for the subject) contributes the observation where the variable X takes it&amp;rsquo;s maximum value for that subject.
R solutions Hadleyverse solutions Using the excellent R package dplyr, we can do this using windowing functions included in dplyr. The following solution is available on StackOverflow, by junkka, and gets around the real possibility that multiple observations might have the same maximum value of X by choosing one of them.</description>
    </item>
    
    <item>
      <title>&#34;LaF&#34;-ing about fixed width formats</title>
      <link>/post/laf-ing-about-fixed-width-formats/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/laf-ing-about-fixed-width-formats/</guid>
      <description>If you have ever worked with US government data or other large datasets, it is likely you have faced fixed-width format data. This format has no delimiters in it; the data look like strings of characters. A separate format file defines which columns of data represent which variables. It seems as if the format is from the punch-card era, but it is quite an efficient format to store large data in (see this StackOverflow discussion).</description>
    </item>
    
    <item>
      <title>Practical Data Science Cookbook</title>
      <link>/post/practical-data-science-cookbook/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/practical-data-science-cookbook/</guid>
      <description>Practical Data Science Cookbook My friends Sean Murphy, Ben Bengfort, Tony Ojeda and I recently published a book, Practical Data Science Cookbook. All of us are heavily involved in developing the data community in the Washington DC metro area, serving on the Board of Directors of Data Community DC. Sean and Ben co-organize the meetup Data Innovation DC and I co-organize the meetup Statistical Programming DC.
Our intention in writing this book is to provide the data practitioner some guidance about how to navigate the data science pipeline, from data acquisition to final reports and data applications.</description>
    </item>
    
    <item>
      <title>The need for documenting functions</title>
      <link>/post/the-need-for-documenting-functions/</link>
      <pubDate>Thu, 22 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/the-need-for-documenting-functions/</guid>
      <description>My current work usually requires me to work on a project until we can submit a research paper, and then move on to a new project. However, 3-6 months down the road, when the reviews for the paper return, it is quite common to have to do some new analyses or re-analyses of the data. At that time, I have to re-visit my code!
One of the common problems I (and I&amp;rsquo;m sure many of us) have is that we tend to hack code and functions with the end in mind, just getting the job done.</description>
    </item>
    
    <item>
      <title>Kaplan-Meier plots using ggplots2 (updated)</title>
      <link>/post/kaplan-meier-plots-using-ggplots2-updated/</link>
      <pubDate>Tue, 01 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/kaplan-meier-plots-using-ggplots2-updated/</guid>
      <description>About 3 years ago I published some code on this blog to draw a Kaplan-Meier plot using ggplot2. Since then, ggplot2 has been updated (from 0.8.9 to 0.9.3.1) and has changed syntactically. Since that post, I have also become comfortable with Git and Github. I have updated the code, edited it for a small error, and published it in a Gist. This gist has two functions, ggkm (basic Kaplan-Meier plot) and ggkmTable (enhanced Kaplan-Meier plot with table showing numbers at risk at various times).</description>
    </item>
    
    <item>
      <title>Pocketbook costs of software</title>
      <link>/post/pocketbook-costs-of-software-2/</link>
      <pubDate>Thu, 23 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/pocketbook-costs-of-software-2/</guid>
      <description>I have always been provided SAS as part of my job, so I never really realized how much it cost. I&amp;rsquo;ve bought Stata before, and of course R :). I recently found out how much a reasonable bundle of SAS modules along with base SAS costs per year per seat, at least under the GSA. I tried finding out how much IBM SPSS is for a comparable bundle, but their web page was &amp;ldquo;not available&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>An enhanced Kaplan-Meier plot, updated</title>
      <link>/post/an-enhanced-kaplan-meier-plot-updated/</link>
      <pubDate>Thu, 01 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/an-enhanced-kaplan-meier-plot-updated/</guid>
      <description>I&amp;rsquo;ve updated the R code for the enhanced K-M plot to include additions and improvements by Gil Thomas and Mark Cowley. Thanks fellows for the feedback and updates. http://statbandit.wordpress.com/2011/03/08/an-enhanced-kaplan-meier-plot/</description>
    </item>
    
    <item>
      <title>RStudio 0.94.92 visited</title>
      <link>/post/rstudio-0-94-92-visited/</link>
      <pubDate>Sat, 30 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/rstudio-0-94-92-visited/</guid>
      <description>I just updated my RStudio version to the latest, v.0.94.92 (will this asymptotically approach 1, or actually get to 1?). It was nice to see the number of improvements the development team has implemented, based I&amp;rsquo;m sure on community feedback. The team has, in my experience, been extraordinarily responsive to user feedback, and I&amp;rsquo;m sure this played a large part in the development path taken by the team.
First and foremost, I was happy to see most of my wants met in this version:</description>
    </item>
    
    <item>
      <title>A word of warning about grep, which and the like</title>
      <link>/post/a-word-of-warning-about-grep-which-and-the-like/</link>
      <pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/a-word-of-warning-about-grep-which-and-the-like/</guid>
      <description>I&amp;rsquo;ve often selected columns or rows of a data frame using grep or which, based on some property. That is inherently sound, but the trouble comes when you wish to remove rows or columns based on that grep or which call, e.g.,
dat &amp;lt;- dat[,-grep(&#39;\\.1&#39;, names(dat))]  which would remove columns with a .1 in the name. This is fine the first time around, but if you forget and re-run the code, grep(&#39;\\.</description>
    </item>
    
    <item>
      <title>SAS, R and categorical variables</title>
      <link>/post/sas-r-and-categorical-variables/</link>
      <pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/sas-r-and-categorical-variables/</guid>
      <description>One of the disappointing problems in SAS (as I need PROC MIXED for some analysis) is to recode categorical variables to have a particular reference category. In R, my usual tool, this is rather easy both to set and to modify using the relevel command available in base R (in the stats package). My understanding is that this is actually easy in SAS for GLM, PHREG and some others, but not in PROC MIXED.</description>
    </item>
    
    <item>
      <title>An enhanced Kaplan-Meier plot</title>
      <link>/post/an-enhanced-kaplan-meier-plot/</link>
      <pubDate>Tue, 08 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/an-enhanced-kaplan-meier-plot/</guid>
      <description>We often see, in publications, a Kaplan-Meier survival plot, with a table of the number of subjects at risk at different time points aligned below the figure. I needed this type of plot (or really, matrices of such plots) for an upcoming publication. Of course, my preferred toolbox was R and the ggplot2 package.
There were other attempts to do this type of plot in ggplot2, mainly by Gary Collins and an anonymous author as seen on the ggplot2 mailing list.</description>
    </item>
    
    <item>
      <title>RStudio: a cut above</title>
      <link>/post/rstudio-a-cut-above/</link>
      <pubDate>Tue, 01 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/rstudio-a-cut-above/</guid>
      <description>As most followers of R-bloggers.com and the Twitter #rstats know by now, RStudio is a new open-source IDE for R that was beta-released yesterday. I have started putting it through its paces within my R workflow, and my impressions are more than favorable. I also tried it out on my home Linux server in server mode.
RStudio is obviously designed by people who actually use R and code in R for their data analyses.</description>
    </item>
    
  </channel>
</rss>
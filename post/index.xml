<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Abhijit Dasgupta</title>
    <link>/post/</link>
    <description>Recent content in Blog on Abhijit Dasgupta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Abhijit Dasgupta</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tidying messy Excel data (Introduction)</title>
      <link>/post/tidying-messy-data-1/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidying-messy-data-1/</guid>
      <description>Personal expressiveness, or how data is stored in a spreadsheet When you get data from a broad research community, the variability in how that data is formatted and stored is truly astonishing. Of course there are the standardized formats that are output from machines, like Next Generation Sequencing and other automated systems. That is a saving grace!
But for smaller data, or data collected in the lab, the possibilities are truly endless!</description>
    </item>
    
    <item>
      <title>Surprising result when exploring Rcpp gallery</title>
      <link>/post/surprising-result-when-exploring-rcpp-gallery/</link>
      <pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/surprising-result-when-exploring-rcpp-gallery/</guid>
      <description>I&amp;rsquo;m starting to incorporate more Rcpp in my R work, and so decided to spend some time exploring the Rcpp Gallery. One example by John Merrill caught my eye. He provides a C++ solution to transforming an list of lists into a data frame, and shows impressive speed savings compared to as.data.frame.
This got me thinking about how I do this operation currently. I tend to rely on the do.call method.</description>
    </item>
    
    <item>
      <title>Quirks about running Rcpp on Windows through RStudio</title>
      <link>/post/quirks-about-running-rcpp-on-windows-through-rstudio/</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/quirks-about-running-rcpp-on-windows-through-rstudio/</guid>
      <description>Quirks about running Rcpp on Windows through RStudio This is a quick note about some tribulations I had running Rcpp (v. 0.12.12) code through RStudio (v. 1.0.143) on a Windows 7 box running R (v. 3.3.2). I also have RTools v. 3.4 installed. I fully admit that this may very well be specific to my box, but I suspect not.
I kept running into problems with Rcpp complaining that (a) RTools wasn&amp;rsquo;t installed, and (b) the C++ compiler couldn&amp;rsquo;t find Rcpp.</description>
    </item>
    
    <item>
      <title>Finding my Dropbox in R</title>
      <link>/post/finding-my-dropbox-in-r/</link>
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/finding-my-dropbox-in-r/</guid>
      <description>I&amp;rsquo;ll often keep non-sensitive data on Dropbox so that I can access it on all my machines without gumming up git. I just wrote a small script to find the Dropbox location on each of my computers automatically. The crucial information is available here, from Dropbox.
My small snippet of code is the following:
if (Sys.info()[&#39;sysname&#39;] == &#39;Darwin&#39;) { info &amp;lt;- RJSONIO::fromJSON( file.path(path.expand(&amp;quot;~&amp;quot;),&#39;.dropbox&#39;,&#39;info.json&#39;)) } if (Sys.info()[&#39;sysname&#39;] == &#39;Windows&#39;) { info &amp;lt;- RJSONIO::fromJSON( ```r if (file.</description>
    </item>
    
    <item>
      <title>Some thoughts on the downsides of current Data Science practice</title>
      <link>/post/some-thoughts-on-the-downsides-of-current-data-science-practice/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/some-thoughts-on-the-downsides-of-current-data-science-practice/</guid>
      <description>Bert Huang has a nice blog talking about poor results of ML/AI algorithms in &amp;ldquo;wild&amp;rdquo; data, which echos some of my experience and thoughts. His conclusions are worth thinking about, IMO.
 1. Big data is complex data. As we go out and collect more data from a finite world, we&#39;re necessarily going to start collecting more and more interdependent data. Back when we had hundreds of people in our databases, it was plausible that none of our data examples were socially connected.</description>
    </item>
    
    <item>
      <title>pandas &#34;transform&#34; using the tidyverse</title>
      <link>/post/pandas-transform-using-the-tidyverse/</link>
      <pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/pandas-transform-using-the-tidyverse/</guid>
      <description>Chris Moffit has a nice blog on how to use the transform function in pandas. He provides some (fake) data on sales and asks the question of what fraction of each order is from each SKU.
Being a R nut and a tidyverse fan, I thought to compare and contrast the code for the pandas version with an implementation using the tidyverse.
First the pandas code:
import pandas as pd dat = pd.</description>
    </item>
    
    <item>
      <title>Changing names in the tidyverse: An example for many regressions</title>
      <link>/post/changing-names-in-the-tidyverse-an-example-for-many-regressions/</link>
      <pubDate>Thu, 09 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/changing-names-in-the-tidyverse-an-example-for-many-regressions/</guid>
      <description>A collaborator posed an interesting R question to me today. She wanted to do several regressions using different outcomes, with models being computed on different strata defined by a combination of experimental design variables. She then just wanted to extract the p-values for the slopes for each of the models, and then filter the strata based on p-value levels.
This seems straighforward, right? Let&amp;rsquo;s set up a toy example:
library(tidyverse) dat &amp;lt;- as_tibble(expand.</description>
    </item>
    
    <item>
      <title>A (much belated) update to plotting Kaplan-Meier curves in the tidyverse</title>
      <link>/post/a-much-belated-update-to-plotting-kaplan-meier-curves-in-the-tidyverse/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-much-belated-update-to-plotting-kaplan-meier-curves-in-the-tidyverse/</guid>
      <description>One of the most popular posts on this blog has been my attempt to create Kaplan-Meier plots with an aligned table of persons-at-risk below it under the ggplot paradigm. That post was last updated 3 years ago. In the interim, Chris Dardis has built upon these attempts to create a much more stable and feature-rich version of this function in his package survMisc; the function is called autoplot.</description>
    </item>
    
    <item>
      <title>Copying tables from R to Outlook</title>
      <link>/post/copying-tables-from-r-to-outlook/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/copying-tables-from-r-to-outlook/</guid>
      <description>I work in an ecosystem that uses Outlook for e-mail. When I have to communicate results with collaborators one of the most frequent tasks I face is to take a tabular output in R (either a summary table or some sort of tabular output) and send it to collaborators in Outlook. One method is certainly to export the table to Excel and then copy the table from there into Outlook. However, I think I prefer another method which works a bit quicker for me.</description>
    </item>
    
    <item>
      <title>A quick exploration of the ReporteRs package</title>
      <link>/post/a-quick-exploration-of-reporters/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/a-quick-exploration-of-reporters/</guid>
      <description>The package ReporteRs has been getting some play on the interwebs this week, though it&amp;rsquo;s actually been around for a while. The nice thing about this package is that it allows writing Word and PowerPoint documents in an OS-independent fashion unlike some earlier packages. It also allows the editing of documents by using bookmarks within the documents.
This quick note is just to remind me that the structure of ReporteRs works beautifully with the piping conventions of magrittr.</description>
    </item>
    
    <item>
      <title>Annotated Facets with ggplot2</title>
      <link>/post/annotated-facets-with-ggplot2/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/annotated-facets-with-ggplot2/</guid>
      <description>I was recently asked to do a panel of grouped boxplots of a continuous variable, with each panel representing a categorical grouping variable. This seems easy enough with ggplot2 and the facet_wrap function, but then my collaborator wanted p-values on the graphs! This post is my approach to the problem.
First of all, one caveat. I&amp;rsquo;m a huge fan of Hadley Wickham&amp;rsquo;s tidyverse and so most of my code will reflect this ethos, including packages and pipes.</description>
    </item>
    
    <item>
      <title>A follow-up to Crowdsourcing Research</title>
      <link>/post/a-follow-up-to-crowdsourcing-research/</link>
      <pubDate>Wed, 10 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/a-follow-up-to-crowdsourcing-research/</guid>
      <description>Last month I published some thoughts on crowdsourcing research, inspired by Anthony Goldbloom&amp;rsquo;s talk at Statistical Programming DC on the Kaggle experience. Today, I found a rather similar discussion on crowdsourcing research (on the online version of the magazine Good) as a potential way to increase the accuracy of scientific research and reducing bias. I think more consideration needs to be made both by academia, funding agencies, journals and consumers of scientific and technological research to break silos and make progress accurate and reproducible, and finding new ways of preserving the profit imperative in technological progress that allows for the sharing and crowdsourcing of knowledge and research progress.</description>
    </item>
    
    <item>
      <title>Crowdsourcing research</title>
      <link>/post/crowdsourcing-research/</link>
      <pubDate>Fri, 15 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/crowdsourcing-research/</guid>
      <description>Last evening, Anthony Goldbloom, the founder of Kaggle.com, gave a very nice talk at a joint Statistical Programming DC/Data Science DC event about the Kaggle experience and what can be learned from the results of their competitions. One of the take away messages was that crowdsourcing data problems to a diligent and motivated group of entrepreneurial data scientists can get you to the threshold of extracting signal and patterns from data far more quickly than if a closed and siloed group of analysts worked on the problem.</description>
    </item>
    
    <item>
      <title>Reading fixed width formats in the Hadleyverse</title>
      <link>/post/reading-fixed-width-formats-in-the-hadleyverse/</link>
      <pubDate>Sun, 19 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/reading-fixed-width-formats-in-the-hadleyverse/</guid>
      <description>This is an update to a previous post on reading fixed width formats in R.
A new addition to the Hadleyverse is the package readr, which includes a function read_fwf to read fixed width format files. I&amp;rsquo;ll compare the LaF approach to the readr approach using the same dataset as before. The variable wt is generated from parsing the Stata load file as before.
I want to read all the data in two columns: DRG and HOSPID.</description>
    </item>
    
    <item>
      <title>Creating new data with max values for each subject</title>
      <link>/post/creating-new-data-with-max-values-for-each-subject-2/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/creating-new-data-with-max-values-for-each-subject-2/</guid>
      <description>We have a data set dat with multiple observations per subject. We want to create a subset of this data such that each subject (with ID giving the unique identifier for the subject) contributes the observation where the variable X takes it&amp;rsquo;s maximum value for that subject.
R solutions Hadleyverse solutions Using the excellent R package dplyr, we can do this using windowing functions included in dplyr. The following solution is available on StackOverflow, by junkka, and gets around the real possibility that multiple observations might have the same maximum value of X by choosing one of them.</description>
    </item>
    
    <item>
      <title>&#34;LaF&#34;-ing about fixed width formats</title>
      <link>/post/laf-ing-about-fixed-width-formats/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/laf-ing-about-fixed-width-formats/</guid>
      <description>If you have ever worked with US government data or other large datasets, it is likely you have faced fixed-width format data. This format has no delimiters in it; the data look like strings of characters. A separate format file defines which columns of data represent which variables. It seems as if the format is from the punch-card era, but it is quite an efficient format to store large data in (see this StackOverflow discussion).</description>
    </item>
    
    <item>
      <title>Munging fixed width formats in Python</title>
      <link>/post/munging-fixed-width-formats-in-python/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/munging-fixed-width-formats-in-python/</guid>
      <description>In a previous post, I described how to munge fixed width format data in R. I also developed Python code for the same use case, which is described in this IPython Notebook. This seems the easiest way to present this given Wordpress.com&amp;rsquo;s restriction on iframe objects.</description>
    </item>
    
    <item>
      <title>Practical Data Science Cookbook</title>
      <link>/post/practical-data-science-cookbook/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/practical-data-science-cookbook/</guid>
      <description>Practical Data Science Cookbook My friends Sean Murphy, Ben Bengfort, Tony Ojeda and I recently published a book, Practical Data Science Cookbook. All of us are heavily involved in developing the data community in the Washington DC metro area, serving on the Board of Directors of Data Community DC. Sean and Ben co-organize the meetup Data Innovation DC and I co-organize the meetup Statistical Programming DC.
Our intention in writing this book is to provide the data practitioner some guidance about how to navigate the data science pipeline, from data acquisition to final reports and data applications.</description>
    </item>
    
    <item>
      <title>The need for documenting functions</title>
      <link>/post/the-need-for-documenting-functions/</link>
      <pubDate>Thu, 22 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/the-need-for-documenting-functions/</guid>
      <description>My current work usually requires me to work on a project until we can submit a research paper, and then move on to a new project. However, 3-6 months down the road, when the reviews for the paper return, it is quite common to have to do some new analyses or re-analyses of the data. At that time, I have to re-visit my code!
One of the common problems I (and I&amp;rsquo;m sure many of us) have is that we tend to hack code and functions with the end in mind, just getting the job done.</description>
    </item>
    
    <item>
      <title>Newer dplyr!!</title>
      <link>/post/newer-dplyr/</link>
      <pubDate>Wed, 21 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/newer-dplyr/</guid>
      <description>Last week Statistical Programming DC had a great meetup with my partner-in-crime Marck Vaisman talking about data.table and dplyr as powerful, fast R tools for data manipulation in R. Today Hadley Wickham announced the release of dplyr v.0.2, which is packed with new features and incorporates the &amp;ldquo;piping&amp;rdquo; syntax from Stefan Holst Bache&amp;rsquo;s magrittr package. I suspect that these developments will change the semantics of working in R, specially during the data munging phase.</description>
    </item>
    
    <item>
      <title>Quick notes on file management in Python</title>
      <link>/post/quick-notes-on-file-management-in-python/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/quick-notes-on-file-management-in-python/</guid>
      <description> This is primarily for my recollection To expand ~ in a path name:
os.path.expanduser(&#39;~&#39;)  To get the size of a directory:
import os def getsize(start&amp;lt;em&amp;gt;path = &#39;.&#39;): totalsize = 0 for dirpath, dirnames, filenames in os.walk(start&amp;lt;/em&amp;gt;path): for f in filenames: fp = os.path.join(dirpath, f) totalsize += os.path.getsize(fp) return totalsize  </description>
    </item>
    
    <item>
      <title>IPython notebooks: the new glue?</title>
      <link>/post/ipython-notebooks-the-new-glue/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/ipython-notebooks-the-new-glue/</guid>
      <description>IPython notebooks have become a defacto standard for presenting Python-based analyses and talks, as evidenced by recent Pycon and PyData events. As anyone who has used them knows, they are great for &amp;ldquo;reproducible research&amp;rdquo;, presentations, and sharing via the nbviewer. There are extensions connecting IPython to R, Octave, Matlab, Mathematica, SQL, among others.
However, the brilliance of the design of IPython is in the modularity of the underlying engine (3 cheers to Fernando Perez and his team).</description>
    </item>
    
    <item>
      <title>A new data-centric incubator project in DC</title>
      <link>/post/a-new-data-centric-incubator-project-in-dc/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/a-new-data-centric-incubator-project-in-dc/</guid>
      <description>District Data Labs is a new endeavor by members of the local data community (myself included) to increase educational outreach about data-related topics through workshops and other media to the local data community.
We want District Data Labs to be an efficient learning resource for people who want to enhance and expand their analytical and technical skill sets. Whether you are a statistician who wants to learn more about programming and creating useful data products or a software engineer that wants to learn how to properly analyze data and use statistical methods to improve the basic analyses you&#39;re doing, we want to equip you with the right skills to better yourself and advance your career.</description>
    </item>
    
    <item>
      <title>Kaplan-Meier plots using ggplots2 (updated)</title>
      <link>/post/kaplan-meier-plots-using-ggplots2-updated/</link>
      <pubDate>Tue, 01 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/kaplan-meier-plots-using-ggplots2-updated/</guid>
      <description>About 3 years ago I published some code on this blog to draw a Kaplan-Meier plot using ggplot2. Since then, ggplot2 has been updated (from 0.8.9 to 0.9.3.1) and has changed syntactically. Since that post, I have also become comfortable with Git and Github. I have updated the code, edited it for a small error, and published it in a Gist. This gist has two functions, ggkm (basic Kaplan-Meier plot) and ggkmTable (enhanced Kaplan-Meier plot with table showing numbers at risk at various times).</description>
    </item>
    
    <item>
      <title>Slidify: Data driven presentations</title>
      <link>/post/slidify-data-driven-presentations/</link>
      <pubDate>Mon, 31 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/slidify-data-driven-presentations/</guid>
      <description>Publishers note: This blog was posted on August 1, 2013 on the Data Community DC blog, http://datacommunitydc.org/blog/2013/08/data-driven-presentations-using-slidify/
Presentations are the stock-in-trade for consultants, managers, teachers, public speakers, and, probably, you. We all have to present our work at some level, to someone we report to or to our peers, or to introduce newcomers to our work. Of course, presentations are passe, so why blog about it? There&amp;rsquo;s already PowerPoint, and maybe Keynote.</description>
    </item>
    
    <item>
      <title>Input data interactively into R</title>
      <link>/post/input-data-interactively-into-r/</link>
      <pubDate>Thu, 30 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/input-data-interactively-into-r/</guid>
      <description>To input data interactively into R, use the function readline:
x &amp;lt;- readline(&amp;quot;What is your answer? &amp;quot;)  </description>
    </item>
    
    <item>
      <title>The many faces of statistics/data science: Can&#39;t we all just get along and learn from each other?</title>
      <link>/post/the-many-faces-of-statisticsdata-science-cant-we-all-just-get-along-and-learn-from-each-other/</link>
      <pubDate>Thu, 12 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/the-many-faces-of-statisticsdata-science-cant-we-all-just-get-along-and-learn-from-each-other/</guid>
      <description>Two blog posts in the last 24 hours caught my attention. First was this post by Jeff Leek noting that there are many fields which are applied statistics by another name (and I&amp;rsquo;d add operations research to his list). The second is an excellent post on Cloudera&amp;rsquo;s blog on constructing case-control studies. It is generally excellent, but has this rather unfortunate (in my view) statement:
Analyzing a case-control study is a problem for a statistician.</description>
    </item>
    
    <item>
      <title>Pocketbook costs of software</title>
      <link>/post/pocketbook-costs-of-software-2/</link>
      <pubDate>Thu, 23 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/pocketbook-costs-of-software-2/</guid>
      <description>I have always been provided SAS as part of my job, so I never really realized how much it cost. I&amp;rsquo;ve bought Stata before, and of course R :). I recently found out how much a reasonable bundle of SAS modules along with base SAS costs per year per seat, at least under the GSA. I tried finding out how much IBM SPSS is for a comparable bundle, but their web page was &amp;ldquo;not available&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Converting images in Python</title>
      <link>/post/converting-images-in-python/</link>
      <pubDate>Thu, 29 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/converting-images-in-python/</guid>
      <description>I had a recent request to convert an entire folder of JPEG images into EPS or similar vector graphics formats. The client was on a Mac, and didn&amp;rsquo;t have ImageMagick. I discovered the Python Image Library  to be enormously useful in this, and allowed me to implement the conversion in around 10 lines of Python code!!!
import Image from glob import glob jpgfiles = glob(&#39;*.jpg&#39;) for u in jpgfiles: out = u.</description>
    </item>
    
    <item>
      <title>An enhanced Kaplan-Meier plot, updated</title>
      <link>/post/an-enhanced-kaplan-meier-plot-updated/</link>
      <pubDate>Thu, 01 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/an-enhanced-kaplan-meier-plot-updated/</guid>
      <description>I&amp;rsquo;ve updated the R code for the enhanced K-M plot to include additions and improvements by Gil Thomas and Mark Cowley. Thanks fellows for the feedback and updates. http://statbandit.wordpress.com/2011/03/08/an-enhanced-kaplan-meier-plot/</description>
    </item>
    
    <item>
      <title>Another application of R getting press</title>
      <link>/post/another-application-of-r-getting-press/</link>
      <pubDate>Thu, 18 Aug 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/another-application-of-r-getting-press/</guid>
      <description>Prof. Atul Butte of Stanford University and colleagues just published two articles in Science Translational Research which got a fair amount of press. In fact I heard about the work on the radio on my commute to work. The research involves developing a computational method which can look at drug-disease interactions based on the NCBI GEO repository to discover potentially new uses for approved drugs. On reading the paper, I realized that their main computational tool is R, in particular the Bioconductor tools as well as pvclust and qvalue.</description>
    </item>
    
    <item>
      <title>RStudio 0.94.92 visited</title>
      <link>/post/rstudio-0-94-92-visited/</link>
      <pubDate>Sat, 30 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/rstudio-0-94-92-visited/</guid>
      <description>I just updated my RStudio version to the latest, v.0.94.92 (will this asymptotically approach 1, or actually get to 1?). It was nice to see the number of improvements the development team has implemented, based I&amp;rsquo;m sure on community feedback. The team has, in my experience, been extraordinarily responsive to user feedback, and I&amp;rsquo;m sure this played a large part in the development path taken by the team.
First and foremost, I was happy to see most of my wants met in this version:</description>
    </item>
    
    <item>
      <title>A ggplot trick to plot different plot types in facets</title>
      <link>/post/a-ggplot-trick-to-plot-different-plot-types-in-facets/</link>
      <pubDate>Fri, 29 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/a-ggplot-trick-to-plot-different-plot-types-in-facets/</guid>
      <description>At the DC useR meetup last week, Marck Vaisman (@wahalulu) showed me a neat trick he&amp;rsquo;d learned to allow different facets in a faceted ggplot graph to have different plot types. The basis for this trick is this blog post in the Learn-R blog. Marck was trying to plot different statistics on our Meetup group&amp;rsquo;s membership on a faceted plot. Some of the variables were amenable to a step plot while others were more amenable to plotting using vertical lines.</description>
    </item>
    
    <item>
      <title>A word of warning about grep, which and the like</title>
      <link>/post/a-word-of-warning-about-grep-which-and-the-like/</link>
      <pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/a-word-of-warning-about-grep-which-and-the-like/</guid>
      <description>I&amp;rsquo;ve often selected columns or rows of a data frame using grep or which, based on some property. That is inherently sound, but the trouble comes when you wish to remove rows or columns based on that grep or which call, e.g.,
dat &amp;lt;- dat[,-grep(&#39;\\.1&#39;, names(dat))]  which would remove columns with a .1 in the name. This is fine the first time around, but if you forget and re-run the code, grep(&#39;\\.</description>
    </item>
    
    <item>
      <title>SAS, R and categorical variables</title>
      <link>/post/sas-r-and-categorical-variables/</link>
      <pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/sas-r-and-categorical-variables/</guid>
      <description>One of the disappointing problems in SAS (as I need PROC MIXED for some analysis) is to recode categorical variables to have a particular reference category. In R, my usual tool, this is rather easy both to set and to modify using the relevel command available in base R (in the stats package). My understanding is that this is actually easy in SAS for GLM, PHREG and some others, but not in PROC MIXED.</description>
    </item>
    
    <item>
      <title>An enhanced Kaplan-Meier plot</title>
      <link>/post/an-enhanced-kaplan-meier-plot/</link>
      <pubDate>Tue, 08 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/an-enhanced-kaplan-meier-plot/</guid>
      <description>We often see, in publications, a Kaplan-Meier survival plot, with a table of the number of subjects at risk at different time points aligned below the figure. I needed this type of plot (or really, matrices of such plots) for an upcoming publication. Of course, my preferred toolbox was R and the ggplot2 package.
There were other attempts to do this type of plot in ggplot2, mainly by Gary Collins and an anonymous author as seen on the ggplot2 mailing list.</description>
    </item>
    
    <item>
      <title>RStudio: a cut above</title>
      <link>/post/rstudio-a-cut-above/</link>
      <pubDate>Tue, 01 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/rstudio-a-cut-above/</guid>
      <description>As most followers of R-bloggers.com and the Twitter #rstats know by now, RStudio is a new open-source IDE for R that was beta-released yesterday. I have started putting it through its paces within my R workflow, and my impressions are more than favorable. I also tried it out on my home Linux server in server mode.
RStudio is obviously designed by people who actually use R and code in R for their data analyses.</description>
    </item>
    
    <item>
      <title>The split-apply-combine paradigm in R</title>
      <link>/post/the-split-apply-combine-paradigm-in-r/</link>
      <pubDate>Fri, 25 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/the-split-apply-combine-paradigm-in-r/</guid>
      <description>Last night at the DC R Users meetup, which was our largest meetup to date, I gave an introductory presentation on data munging, and spent a bit of time on the split-apply-combine paradigm that I use almost daily in my work. I talked mainly about the packages plyr and doBy, which I use a lot now. David Smith posted a link on the Revolution blog to this article by Steve Miller, talking about the virtues of the data.</description>
    </item>
    
    <item>
      <title>ggplot2 joy</title>
      <link>/post/ggplot2-joy/</link>
      <pubDate>Fri, 25 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/ggplot2-joy/</guid>
      <description>I&amp;rsquo;ve been working on a long-term (25+yr) longitudinal study of rheumatoid arthritis with my boss. He just walked in and asked if I could create a plot showing the trajectory of pain scores over time for each subject, separated by educational level (4 groups). Having now worked with ggplot2 for a while, and learning more at the last two DC useR meetups, I realized that I could formulate this in ggplot very easily and in short order.</description>
    </item>
    
    <item>
      <title>Forest plots using R and ggplot2</title>
      <link>/post/forest-plots-using-r-and-ggplot2/</link>
      <pubDate>Mon, 01 Nov 2010 00:00:00 +0000</pubDate>
      
      <guid>/post/forest-plots-using-r-and-ggplot2/</guid>
      <description>Forest plots are most commonly used in reporting meta-analyses, but can be profitably used to summarise the results of a fitted model. They essentially display the estimates for model parameters and their corresponding confidence intervals.
Matt Shotwell just posted a message to the R-help mailing list with his lattice-based solution to the problem of creating forest plots in R. I just figured out how to create a forest plot for a consulting report using ggplot2.</description>
    </item>
    
    <item>
      <title>useR! 2010 done and dusted</title>
      <link>/post/user-2010-done-and-dusted/</link>
      <pubDate>Sat, 24 Jul 2010 00:00:00 +0000</pubDate>
      
      <guid>/post/user-2010-done-and-dusted/</guid>
      <description>The useR! 2010 R users conference just finished up this afternoon with a thought-provoking, controversial, and sometimes hilarious talk by Richard Stallman of GNU fame. It started on Tuesday with great tutorials (I took ones on MICE for multiple imputation and Frank Harrell&amp;rsquo;s excellent regression modeling). In between these bookends was a wonderful conference where I got the chance to put faces to names (from their online presence), make many new friends, hopefully no enemies, and learn quite a bit.</description>
    </item>
    
    <item>
      <title>A small customization of ESS</title>
      <link>/post/a-small-customization-of-ess/</link>
      <pubDate>Fri, 14 May 2010 00:00:00 +0000</pubDate>
      
      <guid>/post/a-small-customization-of-ess/</guid>
      <description>JD Long (at Cerebral Mastication) posted a question on Twitter about an artifact in ESS, where typing &amp;ldquo;&amp;rdquo; gets you &amp;ldquo;&amp;lt;-&amp;ldquo;. This is because in the early days of S+, &amp;ldquo;&amp;rdquo; was an allowed assignment operator, and ESS was developed in that era. Later, it was disallowed in favor of &amp;ldquo;&amp;lt;-&amp;rdquo; and &amp;ldquo;=&amp;rdquo;, so ESS was modified to map &amp;ldquo;_&amp;rdquo; to &amp;ldquo;&amp;lt;-&amp;ldquo;. Now I like the typing convenience of this map, and I don&amp;rsquo;t use underscores in my variable names, so I was fine.</description>
    </item>
    
    <item>
      <title>Quick and dirty parallel processing in R</title>
      <link>/post/quick-and-dirty-parallel-processing-in-r/</link>
      <pubDate>Fri, 30 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>/post/quick-and-dirty-parallel-processing-in-r/</guid>
      <description>R has some powerful tools for parallel processing, which I discovered while searching for ways to fully utilize my 8-core computer at work. What surprised me is how easy it is&amp;hellip;about 6 lines of code, if that. Given that I wasn&amp;rsquo;t allowed to install heavy duty parallel-processing systems like MPICH on the computer, I found that the library SNOW fit the bill nicely through its use of sockets. I also discovered the libraries foreach and iterators, which were released to the community by the development team at Revolution R.</description>
    </item>
    
    <item>
      <title>R amusements</title>
      <link>/post/r-amusements/</link>
      <pubDate>Fri, 05 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/post/r-amusements/</guid>
      <description>On a lark, and to kill a bit of time, I was running the R fortune command looking for references to SAS. Here&amp;rsquo;s what two successive random fortunes turned up. Can there be two more antipodal opinions about the same product? I laughed out loud.
  fortune(&#39;SAS&#39;) There are companies whose yearly license fees to SAS total millions of dollars. Then those companies hire armies of SAS programmers to program an archaic macro language using old statistical methods to produce ugly tables and the worst graphics in the statistical software world.</description>
    </item>
    
    <item>
      <title>Restructured text, a new friend</title>
      <link>/post/restructured-text-a-new-friend/</link>
      <pubDate>Thu, 01 Oct 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/restructured-text-a-new-friend/</guid>
      <description>A problem I alluded to in an earlier post is my inability to transform LaTeX documents generated using Sweave into MS Word documents which are nicely formatted (or even readable!). I took a step back and looked at a rather easy text markup system called Restructured Text (rst). This markup is used extensively for python documentation using the python docutils module and allows transformation of the text file into both LaTex and OpenOffice.</description>
    </item>
    
    <item>
      <title>Benford&#39;s law</title>
      <link>/post/benfords-law/</link>
      <pubDate>Fri, 24 Jul 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/benfords-law/</guid>
      <description>Benford&amp;rsquo;s law has a prominent place in fraud detection analysis, as once again evidenced by the analyses of the Iranian election. Christian Robert, in his wonderful blog, has an entry describing a fascinating property of Benford&amp;rsquo;s law. In some ways it calls into question the usefulness of Benford&amp;rsquo;s law, since it can be satisfied by the product of two unrelated random variables. Actually it calls into question the specificity of Benford&amp;rsquo;s law for fraud detection &amp;ndash; departures from Benford&amp;rsquo;s law might be indicative of fraud (sensitivity), but lack of departure from Benford&amp;rsquo;s law might not mean lack of fraud.</description>
    </item>
    
    <item>
      <title>Python and Excel</title>
      <link>/post/32/</link>
      <pubDate>Fri, 24 Jul 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/32/</guid>
      <description>Excel is unfortunately the lingua franca of data delivery (at least in small amounts) from my collaborators. Often I have to merge several disparate bits of information from several Excel files together. I used to do this using R, since that&amp;rsquo;s what I&amp;rsquo;ve known for many years.
Now, of course, I&amp;rsquo;ve discovered Python!!! I fortunately discovered the excellent xlrd and xlwt packages by John Machin, and the subsequent addition of the xlutils package.</description>
    </item>
    
    <item>
      <title>Python &amp; R for microarrays</title>
      <link>/post/python-r-for-microarrays/</link>
      <pubDate>Sat, 30 May 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/python-r-for-microarrays/</guid>
      <description>Recently I have been processing Affymetrix Exon 1.0 arrays for a collaborator. Our aim is to associate gene expression with chemotherapy-related survival. For this , as an initial pass, I&amp;rsquo;m doing gene-by-gene Cox regression with genes as a single covariate. The problem using R directly is that the data file is HUGE (we&amp;rsquo;re measuring over 1M genes here). I&amp;rsquo;ve adapted my usual R-based pipeline to use python instead. Python is much faster at reading large data sets into memory, as well as looping over the data set.</description>
    </item>
    
    <item>
      <title>Python for HTML templates?</title>
      <link>/post/python-for-html-templates/</link>
      <pubDate>Sat, 30 May 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/python-for-html-templates/</guid>
      <description>One of the products I produce for my microarray clients is web sites which graphically and tabularly summarize the results, including annotation of the transcripts using online databases which are linked to the page. It is not unusual, after pathway analysis, to have 100s of pages. Wonder if there is a python templating framework that can make shorter order of this than I&amp;rsquo;m doing now.</description>
    </item>
    
    <item>
      <title>Floating point pitfalls</title>
      <link>/post/floating-point-pitfalls/</link>
      <pubDate>Sun, 05 Apr 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/floating-point-pitfalls/</guid>
      <description>John D. Cook over at the Endeavour has a series of articles talking about floating-point arithmetic and how it can burn us in computing statistics like the standard deviation, correlation and regression coefficients using the book formulae. Specially enlightening for me was the trick of using the Taylor series expansion of log(1+x) for small values of x, since the error is actually quite small. Fantastic points, John!!
A good summary of his points can be found here</description>
    </item>
    
    <item>
      <title>Easy (?) way to tack Fortran onto Python</title>
      <link>/post/easy-way-to-tack-fortran-onto-python/</link>
      <pubDate>Fri, 06 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/easy-way-to-tack-fortran-onto-python/</guid>
      <description>A recent post on Walking Randomly gave a nice example of using the Python ctypes module to load Fortran functions that have been compiled into a shared library (.so) or DLL (.dll). This seems an easier option than using f2py or pyfort, which have not been working well for me.</description>
    </item>
    
    <item>
      <title>Genz-Bretz multivariate normal in Python</title>
      <link>/post/genz-bretz-multivariate-normal-in-python/</link>
      <pubDate>Fri, 06 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/genz-bretz-multivariate-normal-in-python/</guid>
      <description>I&amp;rsquo;ve been fighting for some time to try and get Genz-Bretz&amp;rsquo;s method for calculating orthant probabilities in multivariate normal distributions imported into Python. I downloaded the fortran code from Alan Genz&amp;rsquo;s site and was unsuccessful in using f2py to link it with Python. However, I discovered the usefulness of the Python_ ctypes_ module in linking with shared libraries (see here). So, I compiled the fortran code using
gfortran mvtdstpack.f -shared -o libmvt.</description>
    </item>
    
    <item>
      <title>New R-Python link</title>
      <link>/post/new-r-python-link/</link>
      <pubDate>Fri, 06 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/new-r-python-link/</guid>
      <description>rpy2 is a nice improvement over rpy which my old friend Greg Warnes had a role in developing. It works pretty seamlessly in Python to allow access to statistical analysis in R. Cool!!</description>
    </item>
    
    <item>
      <title>Workflow with Python and R</title>
      <link>/post/workflow-with-python-and-r/</link>
      <pubDate>Fri, 06 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>/post/workflow-with-python-and-r/</guid>
      <description>I seem to be doing more and more with Python for work over and above using it as a generic scripting language. R has been my workhorse for analysis for a long time (15+ years in various incarnations of S+ and R), but it still has some deficiencies. I&amp;rsquo;m finding Python easier and faster to work with for large data sets. I&amp;rsquo;m also a bit happier with Python&amp;rsquo;s graphical capabilities via matplotlib, which allows dynamic updating of graphs _a la _Matlab, another drawback that R has despite great graphical capabilities.</description>
    </item>
    
  </channel>
</rss>
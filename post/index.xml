<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Abhijit Dasgupta</title>
    <link>/post/</link>
    <description>Recent content in Blog on Abhijit Dasgupta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Abhijit Dasgupta</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Surprising result when exploring Rcpp gallery</title>
      <link>/post/surprising-result-when-exploring-rcpp-gallery/</link>
      <pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/surprising-result-when-exploring-rcpp-gallery/</guid>
      <description>I&amp;rsquo;m starting to incorporate more Rcpp in my R work, and so decided to spend some time exploring the Rcpp Gallery. One example by John Merrill caught my eye. He provides a C++ solution to transforming an list of lists into a data frame, and shows impressive speed savings compared to as.data.frame.
This got me thinking about how I do this operation currently. I tend to rely on the do.call method.</description>
    </item>
    
    <item>
      <title>Quirks about running Rcpp on Windows through RStudio</title>
      <link>/post/quirks-about-running-rcpp-on-windows-through-rstudio/</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/quirks-about-running-rcpp-on-windows-through-rstudio/</guid>
      <description>Quirks about running Rcpp on Windows through RStudio This is a quick note about some tribulations I had running Rcpp (v. 0.12.12) code through RStudio (v. 1.0.143) on a Windows 7 box running R (v. 3.3.2). I also have RTools v. 3.4 installed. I fully admit that this may very well be specific to my box, but I suspect not.
I kept running into problems with Rcpp complaining that (a) RTools wasn&amp;rsquo;t installed, and (b) the C++ compiler couldn&amp;rsquo;t find Rcpp.</description>
    </item>
    
    <item>
      <title>Finding my Dropbox in R</title>
      <link>/post/finding-my-dropbox-in-r/</link>
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/finding-my-dropbox-in-r/</guid>
      <description>I&amp;rsquo;ll often keep non-sensitive data on Dropbox so that I can access it on all my machines without gumming up git. I just wrote a small script to find the Dropbox location on each of my computers automatically. The crucial information is available here, from Dropbox.
My small snippet of code is the following:
if (Sys.info()[&#39;sysname&#39;] == &#39;Darwin&#39;) { info &amp;lt;- RJSONIO::fromJSON( file.path(path.expand(&amp;quot;~&amp;quot;),&#39;.dropbox&#39;,&#39;info.json&#39;)) } if (Sys.info()[&#39;sysname&#39;] == &#39;Windows&#39;) { info &amp;lt;- RJSONIO::fromJSON( if (file.</description>
    </item>
    
    <item>
      <title>Some thoughts on the downsides of current Data Science practice</title>
      <link>/post/some-thoughts-on-the-downsides-of-current-data-science-practice/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/some-thoughts-on-the-downsides-of-current-data-science-practice/</guid>
      <description>Bert Huang has a nice blog talking about poor results of ML/AI algorithms in &amp;ldquo;wild&amp;rdquo; data, which echos some of my experience and thoughts. His conclusions are worth thinking about, IMO.
 1. Big data is complex data. As we go out and collect more data from a finite world, we&#39;re necessarily going to start collecting more and more interdependent data. Back when we had hundreds of people in our databases, it was plausible that none of our data examples were socially connected.</description>
    </item>
    
    <item>
      <title>pandas &#34;transform&#34; using the tidyverse</title>
      <link>/post/pandas-transform-using-the-tidyverse/</link>
      <pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/pandas-transform-using-the-tidyverse/</guid>
      <description>Chris Moffit has a nice blog on how to use the transform function in pandas. He provides some (fake) data on sales and asks the question of what fraction of each order is from each SKU.
Being a R nut and a tidyverse fan, I thought to compare and contrast the code for the pandas version with an implementation using the tidyverse.
First the pandas code:
import pandas as pd dat = pd.</description>
    </item>
    
    <item>
      <title>Changing names in the tidyverse: An example for many regressions</title>
      <link>/post/changing-names-in-the-tidyverse-an-example-for-many-regressions/</link>
      <pubDate>Thu, 09 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/changing-names-in-the-tidyverse-an-example-for-many-regressions/</guid>
      <description>A collaborator posed an interesting R question to me today. She wanted to do several regressions using different outcomes, with models being computed on different strata defined by a combination of experimental design variables. She then just wanted to extract the p-values for the slopes for each of the models, and then filter the strata based on p-value levels.
This seems straighforward, right? Let&amp;rsquo;s set up a toy example:
library(tidyverse) dat &amp;lt;- as_tibble(expand.</description>
    </item>
    
    <item>
      <title>A (much belated) update to plotting Kaplan-Meier curves in the tidyverse</title>
      <link>/post/a-much-belated-update-to-plotting-kaplan-meier-curves-in-the-tidyverse/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/a-much-belated-update-to-plotting-kaplan-meier-curves-in-the-tidyverse/</guid>
      <description>One of the most popular posts on this blog has been my attempt to create Kaplan-Meier plots with an aligned table of persons-at-risk below it under the ggplot paradigm. That post was last updated 3 years ago. In the interim, Chris Dardis has built upon these attempts to create a much more stable and feature-rich version of this function in his package survMisc; the function is called autoplot.</description>
    </item>
    
    <item>
      <title>Copying tables from R to Outlook</title>
      <link>/post/copying-tables-from-r-to-outlook/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/copying-tables-from-r-to-outlook/</guid>
      <description>I work in an ecosystem that uses Outlook for e-mail. When I have to communicate results with collaborators one of the most frequent tasks I face is to take a tabular output in R (either a summary table or some sort of tabular output) and send it to collaborators in Outlook. One method is certainly to export the table to Excel and then copy the table from there into Outlook. However, I think I prefer another method which works a bit quicker for me.</description>
    </item>
    
    <item>
      <title>A quick exploration of the ReporteRs package</title>
      <link>/post/a-quick-exploration-of-reporters/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/a-quick-exploration-of-reporters/</guid>
      <description>The package ReporteRs has been getting some play on the interwebs this week, though it&amp;rsquo;s actually been around for a while. The nice thing about this package is that it allows writing Word and PowerPoint documents in an OS-independent fashion unlike some earlier packages. It also allows the editing of documents by using bookmarks within the documents.
This quick note is just to remind me that the structure of ReporteRs works beautifully with the piping conventions of magrittr.</description>
    </item>
    
    <item>
      <title>Annotated Facets with ggplot2</title>
      <link>/post/annotated-facets-with-ggplot2/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/annotated-facets-with-ggplot2/</guid>
      <description>I was recently asked to do a panel of grouped boxplots of a continuous variable, with each panel representing a categorical grouping variable. This seems easy enough with ggplot2 and the facet_wrap function, but then my collaborator wanted p-values on the graphs! This post is my approach to the problem.
First of all, one caveat. I&amp;rsquo;m a huge fan of Hadley Wickham&amp;rsquo;s tidyverse and so most of my code will reflect this ethos, including packages and pipes.</description>
    </item>
    
    <item>
      <title>A follow-up to Crowdsourcing Research</title>
      <link>/post/a-follow-up-to-crowdsourcing-research/</link>
      <pubDate>Wed, 10 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/a-follow-up-to-crowdsourcing-research/</guid>
      <description>Last month I published some thoughts on crowdsourcing research, inspired by Anthony Goldbloom&amp;rsquo;s talk at Statistical Programming DC on the Kaggle experience. Today, I found a rather similar discussion on crowdsourcing research (on the online version of the magazine Good) as a potential way to increase the accuracy of scientific research and reducing bias. I think more consideration needs to be made both by academia, funding agencies, journals and consumers of scientific and technological research to break silos and make progress accurate and reproducible, and finding new ways of preserving the profit imperative in technological progress that allows for the sharing and crowdsourcing of knowledge and research progress.</description>
    </item>
    
    <item>
      <title>Crowdsourcing research</title>
      <link>/post/crowdsourcing-research/</link>
      <pubDate>Fri, 15 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/crowdsourcing-research/</guid>
      <description>Last evening, Anthony Goldbloom, the founder of Kaggle.com, gave a very nice talk at a joint Statistical Programming DC/Data Science DC event about the Kaggle experience and what can be learned from the results of their competitions. One of the take away messages was that crowdsourcing data problems to a diligent and motivated group of entrepreneurial data scientists can get you to the threshold of extracting signal and patterns from data far more quickly than if a closed and siloed group of analysts worked on the problem.</description>
    </item>
    
    <item>
      <title>Reading fixed width formats in the Hadleyverse</title>
      <link>/post/reading-fixed-width-formats-in-the-hadleyverse/</link>
      <pubDate>Sun, 19 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/reading-fixed-width-formats-in-the-hadleyverse/</guid>
      <description>This is an update to a previous post on reading fixed width formats in R.
A new addition to the Hadleyverse is the package readr, which includes a function read_fwf to read fixed width format files. I&amp;rsquo;ll compare the LaF approach to the readr approach using the same dataset as before. The variable wt is generated from parsing the Stata load file as before.
I want to read all the data in two columns: DRG and HOSPID.</description>
    </item>
    
    <item>
      <title>Creating new data with max values for each subject</title>
      <link>/post/creating-new-data-with-max-values-for-each-subject-2/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/creating-new-data-with-max-values-for-each-subject-2/</guid>
      <description>We have a data set dat with multiple observations per subject. We want to create a subset of this data such that each subject (with ID giving the unique identifier for the subject) contributes the observation where the variable X takes it&amp;rsquo;s maximum value for that subject.
R solutions Hadleyverse solutions Using the excellent R package dplyr, we can do this using windowing functions included in dplyr. The following solution is available on StackOverflow, by junkka, and gets around the real possibility that multiple observations might have the same maximum value of X by choosing one of them.</description>
    </item>
    
    <item>
      <title>&#34;LaF&#34;-ing about fixed width formats</title>
      <link>/post/laf-ing-about-fixed-width-formats/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/laf-ing-about-fixed-width-formats/</guid>
      <description>If you have ever worked with US government data or other large datasets, it is likely you have faced fixed-width format data. This format has no delimiters in it; the data look like strings of characters. A separate format file defines which columns of data represent which variables. It seems as if the format is from the punch-card era, but it is quite an efficient format to store large data in (see this StackOverflow discussion).</description>
    </item>
    
  </channel>
</rss>